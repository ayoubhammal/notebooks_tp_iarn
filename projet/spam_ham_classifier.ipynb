{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3d7e5",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db0cee",
   "metadata": {},
   "source": [
    "Some files are in latin-1 encoding so we are converting them to utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.path.abspath(\"data/spam\")):\n",
    "    try:\n",
    "        with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "        with open(os.path.join(\"data/spam\", filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "for filename in os.listdir(os.path.abspath(\"data/ham\")):\n",
    "    try:\n",
    "        with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "        with open(os.path.join(\"data/ham\", filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c213423",
   "metadata": {},
   "source": [
    "The preprocessing function that handles tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d97aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    file.seek(0)\n",
    "    \n",
    "    # strip metadata\n",
    "    while file.readline() != '\\n':\n",
    "        pass\n",
    "    \n",
    "    # lower case\n",
    "    mail = file.read().lower()\n",
    "    \n",
    "    # html\n",
    "    mail = re.sub('<[^<>]+>', ' ', mail)\n",
    "    \n",
    "    # dollars\n",
    "    mail = mail.replace('$', \"dollar\")\n",
    "    \n",
    "    # urls\n",
    "    mail = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', \n",
    "                  \"httpaddr\",\n",
    "                  mail)\n",
    "    \n",
    "    # emails\n",
    "    mail = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \"emailaddr\", mail)\n",
    "    \n",
    "    # numbers\n",
    "    mail = re.sub(r'\\d+', \"number\", mail)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(mail)\n",
    "    \n",
    "    # stemming\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340320cf",
   "metadata": {},
   "source": [
    "The creation of a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bd6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_vocab = []\n",
    "for filename in os.listdir(os.path.abspath(\"data/spam\")):\n",
    "    with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"utf-8\") as file_content:\n",
    "        tokens = preprocessing(file_content)\n",
    "        raw_vocab += tokens\n",
    "for filename in os.listdir(os.path.abspath(\"data/ham\")):\n",
    "    with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"utf-8\") as file_content:\n",
    "        tokens = preprocessing(file_content)\n",
    "        raw_vocab += tokens\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "raw_vocab = [x for x in raw_vocab if x not in stopwords]\n",
    "print(\"There are\", len(raw_vocab), \"different tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4bdf5",
   "metadata": {},
   "source": [
    "Removing the less frequent words, and keeping only the 10 000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc41344",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "bag_of_words = Counter(raw_vocab)\n",
    "vocab = [x[0] for x in bag_of_words.most_common(vocabulary_size)]\n",
    "print(\"There are\", len(vocab), \"different tokens after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c75a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index + 1\n",
    "    index_to_word[index + 1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_tokens(tokens):\n",
    "    return [word_to_index.get(word, 0) for word in tokens if word in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f56c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(file):\n",
    "    tokens = preprocessing(file)\n",
    "    encoded_tokens = encoding_tokens(tokens)\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailDataset(Dataset):\n",
    "    def __init__(self, \n",
    "            pos_files,\n",
    "            neg_files,\n",
    "            transform=transform,\n",
    "            positive_class_dir=\"data/spam\",\n",
    "            negative_class_dir=\"data/ham\"):\n",
    "        \n",
    "        self.positive_class_dir = positive_class_dir\n",
    "        self.negative_class_dir = negative_class_dir\n",
    "        \n",
    "        self.pos_files = pos_files\n",
    "        self.neg_files = neg_files\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pos_files) + len(self.neg_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.pos_files):\n",
    "            mail_path = os.path.join(self.positive_class_dir, self.pos_files[idx])\n",
    "            label = 1\n",
    "        else:\n",
    "            mail_path = os.path.join(self.negative_class_dir, self.neg_files[idx - len(self.pos_files)])\n",
    "            label = 0\n",
    "        with open(mail_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            mail = transform(file)\n",
    "        return torch.unsqueeze(torch.as_tensor(mail), dim=-1).float(), torch.as_tensor(label).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6d62c",
   "metadata": {},
   "source": [
    "Creating test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_dataset(\n",
    "        test_ratio=0.3,\n",
    "        positive_class_dir=\"data/spam\",\n",
    "        negative_class_dir=\"data/ham\"):\n",
    "    \n",
    "    pos_files = os.listdir(os.path.abspath(positive_class_dir))\n",
    "    neg_files = os.listdir(os.path.abspath(negative_class_dir))\n",
    "    \n",
    "    shuffled_pos_indices = np.random.permutation(len(pos_files))\n",
    "    shuffled_neg_indices = np.random.permutation(len(neg_files))\n",
    "\n",
    "    pos_test_set_size = int(len(pos_files) * test_ratio)\n",
    "    neg_test_set_size = int(len(neg_files) * test_ratio)\n",
    "    \n",
    "    pos_train_indices = shuffled_pos_indices[pos_test_set_size:]\n",
    "    pos_test_indices = shuffled_pos_indices[:pos_test_set_size]\n",
    "    \n",
    "    neg_train_indices = shuffled_neg_indices[neg_test_set_size:]\n",
    "    neg_test_indices = shuffled_neg_indices[:neg_test_set_size]\n",
    "    \n",
    "    pos_train_files = [pos_files[i] for i in pos_train_indices]\n",
    "    neg_train_files = [neg_files[i] for i in neg_train_indices]\n",
    "    \n",
    "    pos_test_files = [pos_files[i] for i in pos_test_indices]\n",
    "    neg_test_files = [neg_files[i] for i in neg_test_indices]\n",
    "    \n",
    "    train_dataset = MailDataset(pos_train_files, neg_train_files)\n",
    "    test_dataset = MailDataset(pos_test_files, neg_test_files)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8164e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    X = [item[0] for item in batch]\n",
    "    y = [item[1] for item in batch]\n",
    "    X_pad = pad_sequence(X, batch_first=True)\n",
    "    return X_pad, torch.as_tensor(y).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6352e",
   "metadata": {},
   "source": [
    "Creating data loaders for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0429e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_train_test_dataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939366a",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abacd40",
   "metadata": {},
   "source": [
    "### Deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d63174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "    def train_step(self, X, y):\n",
    "        self.model.train()\n",
    "        \n",
    "        yhat = self.model(X)\n",
    "        \n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def val_step(self, X, y):\n",
    "        self.model.eval()\n",
    "        \n",
    "        yhat = self.model(X)\n",
    "        \n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    " \n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    " \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "    \n",
    "            self.total_epochs += 1\n",
    "\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "    \n",
    "            scalars = {'training': loss}\n",
    "            if val_loss is not None:\n",
    "                scalars.update({'validation': val_loss})\n",
    "            \n",
    "            print(\"epoch: {0} | training loss: {1:.2f} | validation loss: {2:.2f}\"\n",
    "                  .format(epoch, scalars[\"training\"], scalars[\"validation\"]))\n",
    "                \n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        with torch.no_grad():\n",
    "            y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        self.model.train()\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        if self.val_loader:\n",
    "            plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareModelLSTM(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, n_outputs):\n",
    "        super(SquareModelLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.n_outputs = n_outputs\n",
    "        self.hidden = None\n",
    "        self.cell = None\n",
    "        self.rnn = nn.LSTM(self.n_features,\n",
    "                                 self.hidden_dim,\n",
    "                                 batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_first_output, (self.hidden, self.cell) = \\\n",
    "                                            self.rnn(X)\n",
    "        last_output = batch_first_output[:, -1]\n",
    "        out = self.classifier(last_output)\n",
    "        return out.view(-1, self.n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb794791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SquareModelLSTM(n_features=1, hidden_dim=16, n_outputs=1)\n",
    "optimizer = Adam(lstm_model.parameters(), lr=0.1)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "wrapper = Wrapper(lstm_model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d21f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrapper.set_loaders(train_dataloader, test_dataloader)\n",
    "wrapper.train(n_epochs=20)\n",
    "wrapper.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb1d0f",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_embedding(email):\n",
    "    embedding = np.zeros((1, vocabulary_size))\n",
    "    for word_index in email:\n",
    "        embedding[0, word_index - 1] = 1\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adab0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_full = np.asarray([np.append(email_embedding(x[0]), x[1].item()) for x in train_dataset])\n",
    "train_X, train_y = train_full[:, :-1], train_full[:, -1]\n",
    "\n",
    "test_full = np.asarray([np.append(email_embedding(x[0]), x[1].item()) for x in test_dataset])\n",
    "test_X, test_y = test_full[:, :-1], test_full[:, -1]\n",
    "\n",
    "print(\"train set: x {}, y {}\\ntest set: x {}, y {}\".format(train_X.shape, train_y.shape, test_X.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a05ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_model = SVC(C=0.1, kernel=\"linear\")\n",
    "svm_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fade53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_prediction = svm_model.predict(train_X)\n",
    "test_prediction = svm_model.predict(test_X)\n",
    "\n",
    "train_precision = (train_prediction == train_y).mean() * 100\n",
    "test_precision = (test_prediction == test_y).mean() * 100\n",
    "\n",
    "print(\"train precision: {0:.10f}%\".format(train_precision))\n",
    "print(\"test precision: {0:.10f}%\".format(test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction_scores = svm_model.decision_function(train_X)\n",
    "test_prediction_scores = svm_model.decision_function(test_X)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, test_prediction_scores)\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.ylabel(\"True positive rate (recall)\")\n",
    "plt.xlabel(\"False positive rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
