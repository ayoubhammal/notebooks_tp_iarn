{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61321ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665acbd3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b30459",
   "metadata": {},
   "source": [
    "Some files are in latin-1 encoding so we are converting them to utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.path.abspath(\"data/spam\")):\n",
    "    try:\n",
    "        with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "        with open(os.path.join(\"data/spam\", filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "for filename in os.listdir(os.path.abspath(\"data/ham\")):\n",
    "    try:\n",
    "        with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "        with open(os.path.join(\"data/ham\", filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b282228",
   "metadata": {},
   "source": [
    "The preprocessing function that handles tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155301b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    file.seek(0)\n",
    "    \n",
    "    # strip metadata\n",
    "    while file.readline() != '\\n':\n",
    "        pass\n",
    "    \n",
    "    # lower case\n",
    "    mail = file.read().lower()\n",
    "    \n",
    "    # html\n",
    "    mail = re.sub('<[^<>]+>', ' ', mail)\n",
    "    \n",
    "    # dollars\n",
    "    mail = mail.replace('$', \"dollar\")\n",
    "    \n",
    "    # urls\n",
    "    mail = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', \n",
    "                  \"httpaddr\",\n",
    "                  mail)\n",
    "    \n",
    "    # emails\n",
    "    mail = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \"emailaddr\", mail)\n",
    "    \n",
    "    # numbers\n",
    "    mail = re.sub(r'\\d+', \"number\", mail)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(mail)\n",
    "    \n",
    "    # stemming\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51b018",
   "metadata": {},
   "source": [
    "The creation of a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc93d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_vocab = []\n",
    "for filename in os.listdir(os.path.abspath(\"data/spam\")):\n",
    "    with open(os.path.join(\"data/spam\", filename), \"r\", encoding=\"utf-8\") as file_content:\n",
    "        tokens = preprocessing(file_content)\n",
    "        raw_vocab += tokens\n",
    "for filename in os.listdir(os.path.abspath(\"data/ham\")):\n",
    "    with open(os.path.join(\"data/ham\", filename), \"r\", encoding=\"utf-8\") as file_content:\n",
    "        tokens = preprocessing(file_content)\n",
    "        raw_vocab += tokens\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "raw_vocab = [x for x in raw_vocab if x not in stopwords]\n",
    "print(\"There are\", len(raw_vocab), \"different tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b51eb",
   "metadata": {},
   "source": [
    "Removing the less frequent words, and keeping only the 10 000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c11507",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 500\n",
    "bag_of_words = Counter(raw_vocab)\n",
    "vocab = [x[0] for x in bag_of_words.most_common(vocabulary_size)]\n",
    "print(\"There are\", len(vocab), \"different tokens after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index + 1\n",
    "    index_to_word[index + 1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d07481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_tokens(tokens):\n",
    "    return [word_to_index.get(word, 0) for word in tokens if word in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(file):\n",
    "    tokens = preprocessing(file)\n",
    "    encoded_tokens = encoding_tokens(tokens)\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce029b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailDataset(Dataset):\n",
    "    def __init__(self, \n",
    "            pos_files,\n",
    "            neg_files,\n",
    "            transform=transform,\n",
    "            positive_class_dir=\"data/spam\",\n",
    "            negative_class_dir=\"data/ham\"):\n",
    "        \n",
    "        self.positive_class_dir = positive_class_dir\n",
    "        self.negative_class_dir = negative_class_dir\n",
    "        \n",
    "        self.pos_files = pos_files\n",
    "        self.neg_files = neg_files\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pos_files) + len(self.neg_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.pos_files):\n",
    "            mail_path = os.path.join(self.positive_class_dir, self.pos_files[idx])\n",
    "            label = 1\n",
    "        else:\n",
    "            mail_path = os.path.join(self.negative_class_dir, self.neg_files[idx - len(self.pos_files)])\n",
    "            label = 0\n",
    "        with open(mail_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            mail = [0] + transform(file)\n",
    "        return torch.unsqueeze(torch.as_tensor(mail), dim=-1).float(), torch.as_tensor(label).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76844c12",
   "metadata": {},
   "source": [
    "Creating test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_dataset(\n",
    "        test_ratio=0.3,\n",
    "        positive_class_dir=\"data/spam\",\n",
    "        negative_class_dir=\"data/ham\"):\n",
    "    \n",
    "    pos_files = os.listdir(os.path.abspath(positive_class_dir))\n",
    "    neg_files = os.listdir(os.path.abspath(negative_class_dir))\n",
    "    \n",
    "    shuffled_pos_indices = np.random.permutation(len(pos_files))\n",
    "    shuffled_neg_indices = np.random.permutation(len(neg_files))\n",
    "\n",
    "    pos_test_set_size = int(len(pos_files) * test_ratio)\n",
    "    neg_test_set_size = int(len(neg_files) * test_ratio)\n",
    "    \n",
    "    pos_train_indices = shuffled_pos_indices[pos_test_set_size:]\n",
    "    pos_test_indices = shuffled_pos_indices[:pos_test_set_size]\n",
    "    \n",
    "    neg_train_indices = shuffled_neg_indices[neg_test_set_size:]\n",
    "    neg_test_indices = shuffled_neg_indices[:neg_test_set_size]\n",
    "    \n",
    "    pos_train_files = [pos_files[i] for i in pos_train_indices]\n",
    "    neg_train_files = [neg_files[i] for i in neg_train_indices]\n",
    "    \n",
    "    pos_test_files = [pos_files[i] for i in pos_test_indices]\n",
    "    neg_test_files = [neg_files[i] for i in neg_test_indices]\n",
    "    \n",
    "    train_dataset = MailDataset(pos_train_files, neg_train_files)\n",
    "    test_dataset = MailDataset(pos_test_files, neg_test_files)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c38eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    X = [item[0] for item in batch]\n",
    "    y = [item[1] for item in batch]\n",
    "    X_pad = pad_sequence(X, batch_first=True)\n",
    "    return X_pad, torch.as_tensor(y).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb785ee",
   "metadata": {},
   "source": [
    "Creating data loaders for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1a9bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_train_test_dataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f6827",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb59818",
   "metadata": {},
   "source": [
    "### Deep model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592eeec",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ff83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "    def train_step(self, X, y):\n",
    "        self.model.train()\n",
    "        \n",
    "        yhat = self.model(X)\n",
    "        \n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def val_step(self, X, y):\n",
    "        self.model.eval()\n",
    "        \n",
    "        yhat = self.model(X)\n",
    "        \n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    " \n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    " \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "    \n",
    "            self.total_epochs += 1\n",
    "\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "    \n",
    "            scalars = {'training': loss}\n",
    "            if val_loss is not None:\n",
    "                scalars.update({'validation': val_loss})\n",
    "            \n",
    "            print(\"epoch: {0} | training loss: {1:.2f} | validation loss: {2:.2f}\"\n",
    "                  .format(epoch, scalars[\"training\"], scalars[\"validation\"]))\n",
    "                \n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat_tensor = torch.sigmoid(self.model(x.to(self.device)))\n",
    "        self.model.train()\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        if self.val_loader:\n",
    "            plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, n_outputs):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.n_outputs = n_outputs\n",
    "        self.hidden = None\n",
    "        self.cell = None\n",
    "        self.rnn = nn.LSTM(self.n_features,\n",
    "                                 self.hidden_dim,\n",
    "                                 batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_first_output, (self.hidden, self.cell) = \\\n",
    "                                            self.rnn(X)\n",
    "        last_output = batch_first_output[:, -1]\n",
    "        out = self.classifier(last_output)\n",
    "        return out.view(-1, self.n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0414fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(n_features=1, hidden_dim=128, n_outputs=1)\n",
    "optimizer = Adam(lstm_model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "lstm_wrapper = Wrapper(lstm_model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ed7ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm_wrapper.set_loaders(train_dataloader, test_dataloader)\n",
    "lstm_wrapper.train(n_epochs=5)\n",
    "lstm_wrapper.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c95055",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_full = [(x[0], x[1].item()) for x in test_dataset]\n",
    "\n",
    "test_prediction_scores, test_y = zip(*[(lstm_wrapper.predict(torch.unsqueeze(x, dim=0)).item(), y) \\\n",
    "                                       for x, y in list(test_full) if x.shape[0] > 0])\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, test_prediction_scores)\n",
    "\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "threshold = thresholds[ix]\n",
    "test_prediction = np.asarray([ 1 if x > threshold else 0 for x in test_prediction_scores])\n",
    "print(\"threshold is {0} | accuracy is {0:.2f}%\".format(threshold, (test_prediction==np.asarray(test_y)).mean() * 100))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.ylabel(\"True positive rate (recall)\")\n",
    "plt.xlabel(\"False positive rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fdc11",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_email_embedding(batch):\n",
    "    X = [item[0] for item in batch]\n",
    "    y = [item[1] for item in batch]\n",
    "    \n",
    "    bow = torch.zeros((len(X), vocabulary_size))\n",
    "    \n",
    "    for i, email in enumerate(X):\n",
    "        for word_index in email:\n",
    "            if (word_index.int().item() > 0):\n",
    "                bow[i, word_index.int().item() - 1] = 1\n",
    "    return bow, torch.as_tensor(y).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=ann_email_embedding)\n",
    "bow_test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=ann_email_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff55ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_features, 256, bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.linear2 = nn.Linear(256, 128, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.linear3 = nn.Linear(128, self.n_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.linear1(X)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        \n",
    "        return out.view(-1, self.n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearModel(n_features=vocabulary_size, n_outputs=1)\n",
    "optimizer = Adam(linear_model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "linear_wrapper = Wrapper(lstm_model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79dbc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linear_wrapper.set_loaders(bow_train_dataloader, bow_test_dataloader)\n",
    "linear_wrapper.train(n_epochs=10)\n",
    "linear_wrapper.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb06a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = [ann_email_embedding([(x[0], x[1])]) for x in test_dataset]\n",
    "\n",
    "test_prediction_scores, test_y = zip(*[(linear_wrapper.predict(x).item(), y.item()) \\\n",
    "                                       for x, y in list(test_full) if x.shape[0] > 0])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, test_prediction_scores)\n",
    "\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "threshold = thresholds[ix]\n",
    "test_prediction = np.asarray([ 1 if x > threshold else 0 for x in test_prediction_scores])\n",
    "print(\"threshold is {0} | accuracy is {1:.2f}%\".format(threshold, (test_prediction==np.asarray(test_y)).mean() * 100))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.ylabel(\"True positive rate (recall)\")\n",
    "plt.xlabel(\"False positive rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f722282",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_embedding(email):\n",
    "    embedding = np.zeros((1, vocabulary_size))\n",
    "    for word_index in email:\n",
    "        if (word_index.int().item() > 0):\n",
    "            embedding[0, word_index.int().item() - 1] = 1\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3f33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_full = np.asarray([np.append(email_embedding(x[0]), x[1].item()) for x in train_dataset])\n",
    "train_X, train_y = train_full[:, :-1], train_full[:, -1]\n",
    "\n",
    "test_full = np.asarray([np.append(email_embedding(x[0]), x[1].item()) for x in test_dataset])\n",
    "test_X, test_y = test_full[:, :-1], test_full[:, -1]\n",
    "\n",
    "print(\"train set: x {}, y {}\\ntest set: x {}, y {}\".format(train_X.shape, train_y.shape, test_X.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf75f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_model = SVC(C=0.1, kernel=\"linear\")\n",
    "svm_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3b970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_prediction = svm_model.predict(train_X)\n",
    "test_prediction = svm_model.predict(test_X)\n",
    "\n",
    "train_accuracy = (train_prediction == train_y).mean() * 100\n",
    "test_accuracy = (test_prediction == test_y).mean() * 100\n",
    "\n",
    "print(\"train accuracy: {0:.10f}%\".format(train_accuracy))\n",
    "print(\"test accuracy: {0:.10f}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction_scores = svm_model.decision_function(train_X)\n",
    "test_prediction_scores = svm_model.decision_function(test_X)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, test_prediction_scores)\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.ylabel(\"True positive rate (recall)\")\n",
    "plt.xlabel(\"False positive rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
